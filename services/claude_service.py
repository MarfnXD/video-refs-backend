"""
Servi√ßo para processamento de contexto usando Gemini 3 Pro via Replicate

Gemini 3 Pro (novembro 2025):
- Modelo mais avan√ßado da Google com racioc√≠nio aprofundado
- Suporta multimodal (texto, imagens, v√≠deos, √°udio)
- Thinking level: "high" para m√°xima qualidade de an√°lise
- Substituiu Claude 3.5 Sonnet para melhor integra√ß√£o com Gemini 2.5 Flash
"""
import os
import json
import replicate
from typing import Optional, Dict, List
import logging

logger = logging.getLogger(__name__)

class ClaudeService:
    def __init__(self):
        api_token = os.getenv("REPLICATE_API_TOKEN")
        if not api_token:
            logger.warning("REPLICATE_API_TOKEN n√£o configurada")
            self.client = None
        else:
            self.client = replicate.Client(api_token=api_token)

        # Modelo: Gemini 3 Pro (Google) via Replicate
        # https://replicate.com/google/gemini-3-pro
        self.model_version = "google/gemini-3-pro"

    async def process_context(
        self,
        user_context_raw: str,
        video_title: str = "",
        platform: str = "",
        author: str = "",
        user_categories: List[str] = None,
        user_projects: List[str] = None
    ) -> Optional[Dict]:
        """
        Processa contexto do usu√°rio com Claude 3.5 Sonnet

        Args:
            user_context_raw: Contexto original do usu√°rio
            video_title: T√≠tulo do v√≠deo
            platform: Plataforma (YouTube, Instagram, TikTok)
            author: Autor/canal do v√≠deo
            user_categories: Categorias existentes do usu√°rio
            user_projects: Projetos existentes do usu√°rio

        Returns:
            Dict com contexto processado, tags, categorias e projetos sugeridos
        """
        if not self.client:
            logger.error("Gemini 3 Pro client n√£o inicializado (REPLICATE_API_TOKEN faltando)")
            return None

        if not user_context_raw or not user_context_raw.strip():
            logger.warning("Contexto vazio, pulando processamento")
            return None

        try:
            logger.info(f"üß† Processando contexto com Gemini 3 Pro via Replicate...")

            # Preparar listas para o prompt
            categories_list = ", ".join(user_categories) if user_categories else "Nenhuma categoria ainda"
            projects_list = ", ".join(user_projects) if user_projects else "Nenhum projeto ainda"

            # Montar prompt
            prompt = self._build_prompt(
                user_context_raw,
                video_title,
                platform,
                author,
                categories_list,
                projects_list
            )

            # Chamar Gemini 3 Pro via Replicate
            # https://replicate.com/google/gemini-3-pro
            logger.info(f"üîÆ Chamando Gemini 3 Pro com thinking_level=high...")
            output = self.client.run(
                self.model_version,
                input={
                    "images": [],  # Explicitamente incluir (mesmo vazio)
                    "max_output_tokens": 65535,  # Limite m√°ximo (teste manual bem-sucedido)
                    "prompt": prompt,
                    "temperature": 1,  # Gemini 3: recomenda manter em 1.0 (default)
                    "thinking_level": "high",  # M√°ximo racioc√≠nio (espec√≠fico Gemini 3)
                    "top_p": 0.95,  # Gemini 3 default
                    "videos": []  # Explicitamente incluir (mesmo vazio)
                }
            )

            # DEBUG: Log do tipo de output retornado
            logger.debug(f"üîç DEBUG - Tipo de output: {type(output)}")
            logger.debug(f"üîç DEBUG - Output object: {output}")

            # Extrair resposta (output √© um iterator de strings segundo schema)
            response_text = ""
            chunk_count = 0
            for chunk in output:
                chunk_count += 1
                logger.debug(f"üîç DEBUG - Chunk {chunk_count}: {type(chunk)} = {repr(chunk)[:200]}")
                response_text += chunk

            logger.debug(f"‚úÖ DEBUG - Total de chunks: {chunk_count}, texto final: {len(response_text)} chars")
            logger.debug(f"Resposta Gemini 3 Pro: {response_text[:500]}...")

            # Limpar markdown code blocks se houver (Gemini 3 retorna ```json\n...\n```)
            json_text = response_text.strip()
            if json_text.startswith("```"):
                logger.debug("üßπ Removendo markdown code fence do JSON...")
                lines = json_text.split('\n')
                # Remover primeira linha (```json ou ```)
                if lines[0].startswith('```'):
                    lines = lines[1:]
                # Remover √∫ltima linha se for apenas ```
                if lines and lines[-1].strip() == '```':
                    lines = lines[:-1]
                json_text = '\n'.join(lines).strip()
                logger.debug(f"‚úÖ Markdown removido - JSON limpo: {len(json_text)} chars")

            # Se tem texto extra depois do JSON, encontrar onde o JSON termina
            if '\n\n' in json_text:
                # Tenta encontrar onde termina o JSON (procura por } seguido de linha vazia)
                lines = json_text.split('\n')
                json_lines = []
                brace_count = 0
                for line in lines:
                    json_lines.append(line)
                    brace_count += line.count('{') - line.count('}')
                    # Quando chaves balancearem em zero, JSON completo
                    if brace_count == 0 and '{' in ''.join(json_lines):
                        break
                json_text = '\n'.join(json_lines)

            result = json.loads(json_text)

            logger.info(f"‚úÖ Processamento conclu√≠do: {len(result.get('tags', []))} tags, {len(result.get('suggested_categories', []))} categorias")
            return result

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erro ao parsear JSON da resposta Gemini 3 Pro: {str(e)}")
            logger.error(f"Resposta raw (primeiros 1000 chars): {response_text[:1000]}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar contexto com Gemini 3 Pro: {str(e)}")
            return None

    def _build_prompt(
        self,
        user_context: str,
        video_title: str,
        platform: str,
        author: str,
        categories_list: str,
        projects_list: str
    ) -> str:
        """Constr√≥i o prompt para Claude"""
        return f"""Voc√™ √© um assistente especializado em organizar refer√™ncias criativas para profissionais de v√≠deo/marketing.

CONTEXTO DO USU√ÅRIO:
"{user_context}"

METADADOS DO V√çDEO:
- T√≠tulo: {video_title or "N/A"}
- Plataforma: {platform or "N/A"}
- Canal/Autor: {author or "N/A"}

CATEGORIAS EXISTENTES DO USU√ÅRIO:
{categories_list}

PROJETOS EXISTENTES DO USU√ÅRIO:
{projects_list}

TAREFA:
1. Melhore a nota do usu√°rio (mantenha o sentido, mas torne mais claro e estruturado)
2. Extraia tags relevantes (m√°ximo 5)
3. Sugira M√öLTIPLAS categorias (1-3) dentre as padr√µes ou crie novas:
   PADR√ïES:
   - Ideia de Conte√∫do
   - T√©cnica de Edi√ß√£o
   - Refer√™ncia Visual
   - √Åudio/M√∫sica
   - Mec√¢nica de Campanha
   - Ferramenta/Software
   - Storytelling
   - Outro

   Se o contexto mencionar algo espec√≠fico n√£o coberto, sugira nova categoria.
   Priorize categorias que o usu√°rio j√° usa (se aplic√°vel).

4. Extraia projetos mencionados ou sugira projetos existentes relevantes (m√°ximo 2)
   - Se usu√°rio menciona "para cliente X", "campanha Y", extraia como projeto
   - Se contexto √© similar a projetos existentes, sugira

5. Identifique palavras-chave para busca futura

RETORNE APENAS JSON (sem markdown, sem explica√ß√µes):
{{
  "context_processed": "string (contexto melhorado)",
  "tags": ["tag1", "tag2"],
  "suggested_categories": ["categoria1", "categoria2"],
  "suggested_projects": ["projeto1"],
  "search_keywords": ["keyword1", "keyword2"],
  "confidence": "high|medium|low (qu√£o confiante nas sugest√µes)"
}}"""

    async def process_metadata_auto(
        self,
        title: str,
        description: str = "",
        hashtags: List[str] = None,
        top_comments: List[Dict] = None,
        video_transcript: str = "",
        visual_analysis: str = "",
        user_context: str = ""
    ) -> Optional[Dict]:
        """
        Processa metadados do v√≠deo automaticamente (com ou sem contexto do usu√°rio)

        Args:
            title: T√≠tulo do v√≠deo
            description: Descri√ß√£o do v√≠deo
            hashtags: Lista de hashtags
            top_comments: Lista de coment√°rios top [{text, likes, author}]
            video_transcript: Transcri√ß√£o do √°udio (Whisper API)
            visual_analysis: An√°lise visual dos frames (GPT-4 Vision)
            user_context: Contexto manual do usu√°rio (opcional - peso m√°ximo se fornecido)

        Returns:
            Dict com auto_description, auto_tags, auto_categories
        """
        if not self.client:
            logger.error("Gemini 3 Pro client n√£o inicializado (REPLICATE_API_TOKEN faltando)")
            return None

        try:
            logger.info(f"ü§ñ Processando metadados automaticamente com Gemini 3 Pro...")

            # DEBUG: Log dos par√¢metros recebidos
            logger.debug(f"üìä Par√¢metros recebidos:")
            logger.debug(f"   title: {len(title)} chars")
            logger.debug(f"   visual_analysis: {len(visual_analysis) if visual_analysis else 0} chars")
            logger.debug(f"   user_context: {len(user_context) if user_context else 0} chars")
            if visual_analysis:
                logger.debug(f"   visual_analysis preview: {visual_analysis[:200]}...")

            # Preparar dados
            hashtags_str = ", ".join(hashtags) if hashtags else "Nenhuma"

            # Filtrar e formatar coment√°rios (guardamos os filtrados para retornar)
            filtered_comments_list = []
            if top_comments:
                filtered_comments_list = self._filter_and_prioritize_comments(top_comments, max_count=50)
                comments_str = self._format_filtered_comments(filtered_comments_list)
            else:
                comments_str = "Nenhum"

            # Montar prompt (com transcri√ß√£o, an√°lise visual e contexto do usu√°rio)
            prompt = self._build_auto_prompt(
                title, description, hashtags_str, comments_str,
                video_transcript, visual_analysis, user_context
            )

            # DEBUG: Log do prompt sendo enviado
            logger.debug(f"üìù DEBUG - Prompt length: {len(prompt)} chars")
            logger.debug(f"üìù DEBUG - Prompt preview (primeiros 500 chars):\n{prompt[:500]}")

            # Chamar Gemini 3 Pro via Replicate
            logger.info(f"üîÆ Chamando Gemini 3 Pro (auto) com thinking_level=high...")
            logger.debug(f"üîÆ DEBUG - Input parameters: max_output_tokens=65535, temperature=1, top_p=0.95, thinking_level=high, images=[], videos=[]")

            output = self.client.run(
                self.model_version,
                input={
                    "images": [],  # Explicitamente incluir (mesmo vazio)
                    "max_output_tokens": 65535,  # Limite m√°ximo (teste manual bem-sucedido)
                    "prompt": prompt,
                    "temperature": 1,  # Gemini 3: recomenda manter em 1.0 (default)
                    "thinking_level": "high",  # M√°ximo racioc√≠nio (espec√≠fico Gemini 3)
                    "top_p": 0.95,  # Gemini 3 default
                    "videos": []  # Explicitamente incluir (mesmo vazio)
                }
            )

            # DEBUG: Log do tipo de output retornado
            logger.debug(f"üîç DEBUG (auto) - Tipo de output: {type(output)}")
            logger.debug(f"üîç DEBUG (auto) - Output object: {output}")

            # Extrair resposta (output √© um iterator de strings segundo schema)
            response_text = ""
            chunk_count = 0
            for chunk in output:
                chunk_count += 1
                logger.debug(f"üîç DEBUG (auto) - Chunk {chunk_count}: {type(chunk)} = {repr(chunk)[:200]}")
                response_text += chunk

            logger.debug(f"‚úÖ DEBUG (auto) - Total de chunks: {chunk_count}, texto final: {len(response_text)} chars")
            logger.debug(f"Resposta Gemini 3 Pro (auto): {response_text[:500]}...")

            # Limpar markdown code blocks se houver (Gemini 3 retorna ```json\n...\n```)
            json_text = response_text.strip()
            if json_text.startswith("```"):
                logger.debug("üßπ Removendo markdown code fence do JSON...")
                lines = json_text.split('\n')
                # Remover primeira linha (```json ou ```)
                if lines[0].startswith('```'):
                    lines = lines[1:]
                # Remover √∫ltima linha se for apenas ```
                if lines and lines[-1].strip() == '```':
                    lines = lines[:-1]
                json_text = '\n'.join(lines).strip()
                logger.debug(f"‚úÖ Markdown removido - JSON limpo: {len(json_text)} chars")

            # Se tem texto extra depois do JSON, encontrar onde o JSON termina
            if '\n\n' in json_text:
                # Tenta encontrar onde termina o JSON (procura por } seguido de linha vazia)
                lines = json_text.split('\n')
                json_lines = []
                brace_count = 0
                for line in lines:
                    json_lines.append(line)
                    brace_count += line.count('{') - line.count('}')
                    # Quando chaves balancearem em zero, JSON completo
                    if brace_count == 0 and '{' in ''.join(json_lines):
                        break
                json_text = '\n'.join(json_lines)

            result = json.loads(json_text)

            # Adicionar coment√°rios filtrados ao resultado
            result['filtered_comments'] = filtered_comments_list

            logger.info(f"‚úÖ Processamento autom√°tico conclu√≠do: {len(result.get('auto_tags', []))} tags, {len(filtered_comments_list)} coment√°rios filtrados")
            return result

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erro ao parsear JSON da resposta Gemini 3 Pro (auto): {str(e)}")
            logger.error(f"Resposta raw (primeiros 1000 chars): {response_text[:1000] if response_text else 'VAZIO'}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar metadados automaticamente com Gemini 3 Pro: {str(e)}")
            return None

    def _is_generic_comment(self, text: str) -> bool:
        """Detecta coment√°rios gen√©ricos/irrelevantes"""
        if not text or len(text.strip()) < 3:
            return True

        text_lower = text.lower().strip()

        # Lista de padr√µes gen√©ricos (portugu√™s + ingl√™s + espanhol)
        generic_patterns = [
            # Portugu√™s
            "top", "kkk", "kkkk", "primeiro", "segunda", "primeir√£o",
            "like", "mt bom", "demais", "foda", "incrivel", "show",
            # Ingl√™s
            "first", "second", "nice", "cool", "wow", "great", "amazing",
            "love it", "love this", "awesome", "fire", "lit",
            # Espanhol
            "primero", "que bueno", "increible",
            # Emojis/s√≠mbolos comuns
            "‚ù§", "üî•", "üòç", "üëè", "üíØ", "üòÇ", "ü§£", "üëç",
        ]

        # Verifica se √© APENAS emojis ou APENAS uma palavra gen√©rica
        for pattern in generic_patterns:
            if text_lower == pattern or text_lower.replace(" ", "") == pattern.replace(" ", ""):
                return True

        # Se tem menos de 5 caracteres e cont√©m emojis, provavelmente √© gen√©rico
        if len(text_lower) < 5 and any(char in text for char in "‚ù§üî•üòçüëèüíØüòÇü§£üëç"):
            return True

        return False

    def _filter_and_prioritize_comments(self, comments: List[Dict], max_count: int = 50) -> List[Dict]:
        """Filtra coment√°rios gen√©ricos e prioriza por relev√¢ncia"""
        if not comments:
            return []

        # Filtrar coment√°rios gen√©ricos
        filtered = [c for c in comments if not self._is_generic_comment(c.get('text', ''))]

        # Se filtrou tudo, usa os originais (melhor coment√°rios gen√©ricos que nenhum)
        if not filtered:
            filtered = comments

        # Ordenar por likes (coment√°rios com mais likes primeiro)
        sorted_comments = sorted(filtered, key=lambda x: x.get('likes', 0), reverse=True)

        # Retornar os top N
        return sorted_comments[:max_count]

    def _format_filtered_comments(self, filtered_comments: List[Dict]) -> str:
        """Formata coment√°rios j√° filtrados para o prompt"""
        if not filtered_comments:
            return "Nenhum coment√°rio relevante"

        formatted = []
        for i, comment in enumerate(filtered_comments, 1):
            text = comment.get('text', '')
            likes = comment.get('likes', 0)
            formatted.append(f"  {i}. \"{text}\" ({likes} likes)")

        return "\n".join(formatted)

    def _format_comments(self, comments: List[Dict]) -> str:
        """Formata coment√°rios para o prompt (com filtro inteligente)"""
        if not comments:
            return "Nenhum"

        # Filtrar e priorizar (50 melhores coment√°rios)
        filtered_comments = self._filter_and_prioritize_comments(comments, max_count=50)

        if not filtered_comments:
            return "Nenhum coment√°rio relevante"

        formatted = []
        for i, comment in enumerate(filtered_comments, 1):
            text = comment.get('text', '')
            likes = comment.get('likes', 0)
            formatted.append(f"  {i}. \"{text}\" ({likes} likes)")

        return "\n".join(formatted)

    def _build_auto_prompt(
        self,
        title: str,
        description: str,
        hashtags_str: str,
        comments_str: str,
        video_transcript: str = "",
        visual_analysis: str = "",
        user_context: str = ""
    ) -> str:
        """Constr√≥i o prompt para processamento autom√°tico de metadados"""
        return f"""Voc√™ √© um assistente que extrai tags e categorias de v√≠deos de refer√™ncia criativa.

DADOS DISPON√çVEIS (em ordem de confiabilidade):

üé¨ AN√ÅLISE VISUAL DO V√çDEO (Gemini Flash 2.5 - FONTE PRIM√ÅRIA):
{visual_analysis if visual_analysis else 'N√£o dispon√≠vel'}
‚ö†Ô∏è Esta √© a VERDADE ABSOLUTA - o Gemini VIU o v√≠deo completo e descreveu objetivamente.

üë§ CONTEXTO DO USU√ÅRIO (2¬™ prioridade - se fornecido):
{user_context if user_context else 'N√£o fornecido'}
‚ö†Ô∏è Se fornecido, o usu√°rio sabe POR QUE salvou - considere fortemente na an√°lise.

üìä METADADOS EXTERNOS (Apify - VALIDAR CRITICAMENTE):
- T√≠tulo: "{title}"
- Descri√ß√£o: "{description or 'N√£o dispon√≠vel'}"
- Hashtags: {hashtags_str}
- Coment√°rios: {comments_str}

‚ö†Ô∏è IMPORTANTE: Estes metadados podem estar ERRADOS ou ser CLICKBAIT.
Voc√™ DEVE validar se fazem sentido com o que o Gemini descreveu.

INSTRU√á√ïES DE VALIDA√á√ÉO:

1. **Comece pela an√°lise do Gemini** - ela √© a fonte prim√°ria de verdade
2. **Se houver contexto do usu√°rio**, considere fortemente (ele sabe por que salvou)
3. **Valide os metadados Apify criticamente**:
   - O t√≠tulo/descri√ß√£o BATE com o que o Gemini descreveu?
   - Os coment√°rios fazem sentido com a an√°lise visual?
   - As hashtags s√£o relevantes ou apenas spam/clickbait?
4. **IGNORE dados que contradizem o Gemini**:
   - Exemplo: Gemini diz "anima√ß√£o 3D de Monsters Inc" mas t√≠tulo diz "marketing com AI"
   - Neste caso: IGNORE o t√≠tulo, baseie-se NO QUE REALMENTE EST√Å NO V√çDEO
5. **Use apenas dados que AGREGAM √† an√°lise do Gemini**:
   - Se t√≠tulo/coment√°rios adicionam contexto √∫til ‚Üí use
   - Se s√£o gen√©ricos/contradit√≥rios/clickbait ‚Üí ignore

REGRAS DE EXTRA√á√ÉO:

- Tags devem refletir o que o Gemini VIU (n√£o o que t√≠tulo/coment√°rios dizem)
- Se Gemini menciona "anima√ß√£o 3D" ‚Üí tag: "3d-animation"
- Se Gemini menciona t√©cnica espec√≠fica (jump cut, slow motion) ‚Üí tag com a t√©cnica
- Categorias devem fazer sentido com CONTE√öDO REAL do v√≠deo
- N√ÉO force FOOH/CGI se Gemini n√£o mencionar explicitamente outdoor/billboard CGI

HIERARQUIA FINAL (ordem de prioridade):

1Ô∏è‚É£ An√°lise Visual Gemini = VERDADE ABSOLUTA (ele VIU o v√≠deo!)
2Ô∏è‚É£ Contexto do usu√°rio = 2¬™ prioridade (se fornecido)
3Ô∏è‚É£ Metadados Apify = Use APENAS se validarem com Gemini

CATEGORIAS DISPON√çVEIS:
- T√©cnica de Edi√ß√£o
- Refer√™ncia Visual
- Ideia de Conte√∫do
- √Åudio/M√∫sica
- Ferramenta/Software
- Mec√¢nica de Campanha
- Storytelling
- Tutorial
- Case de Sucesso
- FOOH / CGI Advertising (APENAS se Gemini mencionar outdoor/billboard CGI em ambiente real)
- Outro

RETORNE APENAS JSON:
{{
  "auto_description": "string (baseado PRINCIPALMENTE na an√°lise Gemini)",
  "auto_tags": ["tag1", "tag2", "tag3", "tag4", "tag5"],
  "auto_categories": ["categoria1", "categoria2"],
  "confidence": "high|medium|low",
  "relevance_score": 0.0-1.0
}}"""

    async def process_metadata_with_gemini(
        self,
        title: str,
        description: str = "",
        hashtags: List[str] = None,
        top_comments: List[Dict] = None,
        gemini_analysis: Dict = None,
        user_context: str = ""
    ) -> Optional[Dict]:
        """
        **ATUALIZADO - GEMINI FLASH 2.5 TIMELINE FORMAT**

        Processa metadados do v√≠deo usando descri√ß√£o timeline do Gemini Flash 2.5

        Args:
            title: T√≠tulo do v√≠deo
            description: Descri√ß√£o do v√≠deo
            hashtags: Lista de hashtags
            top_comments: Lista de coment√°rios top [{text, likes, author}]
            gemini_analysis: Dict retornado pelo Gemini (formato novo):
                - transcript: Descri√ß√£o timeline objetiva do v√≠deo
                - visual_analysis: Mesmo conte√∫do (compatibilidade)
                - language: Idioma detectado
                - confidence: 0.0-1.0
            user_context: Contexto manual do usu√°rio (opcional - peso m√°ximo)

        Returns:
            Dict com auto_description, auto_tags, auto_categories, relevance_score
        """
        if not self.client:
            logger.error("‚ùå Claude client n√£o inicializado (REPLICATE_API_TOKEN faltando)")
            return None

        try:
            logger.info(f"ü§ñ Processando metadados com Gemini timeline...")

            # DEBUG: Log gemini_analysis recebido
            if gemini_analysis:
                logger.debug(f"üìä Gemini analysis recebido: {list(gemini_analysis.keys())}")
                logger.debug(f"   transcript: {len(gemini_analysis.get('transcript', ''))} chars")
                logger.debug(f"   visual_analysis: {len(gemini_analysis.get('visual_analysis', ''))} chars")
            else:
                logger.warning(f"‚ö†Ô∏è gemini_analysis √© None!")

            # Preparar dados
            hashtags_str = ", ".join(hashtags) if hashtags else "Nenhuma"

            # Filtrar coment√°rios
            filtered_comments_list = []
            if top_comments:
                filtered_comments_list = self._filter_and_prioritize_comments(top_comments, max_count=50)
                comments_str = self._format_filtered_comments(filtered_comments_list)
            else:
                comments_str = "Nenhum"

            # Extrair descri√ß√£o timeline do Gemini (formato novo - texto livre)
            gemini_timeline = gemini_analysis.get('visual_analysis', '') if gemini_analysis else ''

            # DEBUG: Log do timeline extra√≠do
            logger.debug(f"üìù Timeline extra√≠do: {len(gemini_timeline)} chars")
            if gemini_timeline:
                logger.debug(f"   Preview: {gemini_timeline[:200]}...")

            # Usar m√©todo process_metadata_auto (que j√° foi atualizado)
            # passando a descri√ß√£o do Gemini como visual_analysis
            return await self.process_metadata_auto(
                title=title,
                description=description,
                hashtags=hashtags,
                top_comments=top_comments,
                video_transcript="",  # Gemini j√° inclui √°udio na timeline
                visual_analysis=gemini_timeline,
                user_context=user_context
            )

        except Exception as e:
            logger.error(f"‚ùå Erro ao processar metadados com Gemini: {str(e)}")
            return None

    def is_available(self) -> bool:
        """Verifica se o servi√ßo est√° dispon√≠vel"""
        return self.client is not None

# Inst√¢ncia global do servi√ßo
claude_service = ClaudeService()