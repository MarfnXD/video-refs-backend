"""
Celery Tasks - Background Jobs
Tasks ass√≠ncronas processadas pelos workers
"""
from celery_app import celery_app
from celery import chain, group
from typing import Optional, List, Dict
import logging
import os
import tempfile
from datetime import datetime
import asyncio
import time
from dotenv import load_dotenv

# Carregar env vars
load_dotenv()

# Services
from services.apify_service import ApifyService
from services.gemini_service import gemini_service
from services.claude_service import claude_service
from services.thumbnail_service import ThumbnailService
from services.embedding_service import embedding_service
from supabase import create_client, Client

logger = logging.getLogger(__name__)

# ============================================================================
# HELPER: Logging com timing
# ============================================================================

class TaskTimer:
    """Helper para medir dura√ß√£o de tasks e criar logs consolidados"""

    def __init__(self, task_name: str, bookmark_id: str):
        self.task_name = task_name
        self.bookmark_id = bookmark_id
        self.start_time = None

    def start(self):
        """Inicia timer e loga in√≠cio da task"""
        self.start_time = time.time()
        logger.info(f"üìä [{self.task_name}] {self.bookmark_id} - IN√çCIO")

    def success(self, **details):
        """Loga sucesso com dura√ß√£o e detalhes"""
        elapsed = time.time() - self.start_time if self.start_time else 0
        details_str = " | ".join([f"{k}: {v}" for k, v in details.items()])
        logger.info(
            f"‚úÖ [{self.task_name}] {self.bookmark_id} - SUCESSO | "
            f"{details_str} | {elapsed:.1f}s"
        )

    def error(self, error_msg: str):
        """Loga erro com dura√ß√£o"""
        elapsed = time.time() - self.start_time if self.start_time else 0
        logger.error(
            f"‚ùå [{self.task_name}] {self.bookmark_id} - ERRO | "
            f"{error_msg} | {elapsed:.1f}s"
        )

# Inicializar services
apify_service = ApifyService()

# Supabase client
supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
supabase_client: Client = create_client(supabase_url, supabase_key) if supabase_url and supabase_key else None
thumbnail_service = ThumbnailService(supabase_client) if supabase_client else None


# ============================================================================
# TASK PRINCIPAL: Processar bookmark completo
# ============================================================================

@celery_app.task(bind=True, name="tasks.process_bookmark_complete_task")
def process_bookmark_complete_task(
    self,
    bookmark_id: str,
    url: str,
    user_id: str,
    extract_metadata: bool = True,
    analyze_video: bool = True,
    process_ai: bool = True,
    upload_to_cloud: bool = False
):
    """
    Task principal que orquestra TODO o processamento de um bookmark.

    Executa em cadeia (chain):
    1. Extrai metadados (Apify)
    2. Upload pra cloud (se upload_to_cloud=True) - ANTES da an√°lise!
    3. Analisa v√≠deo com Gemini 2.5 Flash (usa cloud_video_url)
    4. Processa com Gemini 3.0 Pro (tags, categorias, descri√ß√£o)
    5. Cleanup e notifica√ß√£o

    Args:
        bookmark_id: UUID do bookmark no Supabase
        url: URL do v√≠deo (YouTube/Instagram/TikTok)
        user_id: UUID do usu√°rio
        extract_metadata: Extrair metadados (t√≠tulo, descri√ß√£o, etc)
        analyze_video: Analisar v√≠deo com Gemini 2.5 Flash (√°udio + visual)
        process_ai: Processar com Gemini 3.0 Pro (tags/categorias)
        upload_to_cloud: Fazer upload do v√≠deo pra Supabase Storage

    Returns:
        dict: Resultado final do processamento
    """
    # Extrair dom√≠nio da URL para log
    url_domain = url.split('/')[2] if '/' in url else url[:30]
    logger.info(f"üöÄ [PIPELINE] {bookmark_id} - IN√çCIO | URL: {url_domain}")

    # Atualizar status no Supabase: queued ‚Üí processing
    update_bookmark_status(bookmark_id, "processing", self.request.id)

    try:
        # Criar pipeline de tasks (executam em sequ√™ncia)
        # IMPORTANTE: Tasks encadeadas recebem resultado da anterior via primeiro par√¢metro
        # Formato: task.s() = signature imut√°vel (par√¢metros fixos)
        #          task.signature() ou task.si() = signature imut√°vel completa

        # 1. Extra√ß√£o de metadados (sempre roda primeiro)
        if extract_metadata:
            # Primeira task: passa par√¢metros expl√≠citos
            workflow = extract_metadata_task.s(bookmark_id, url, user_id)

            # 2. Upload pra cloud (ANTES da an√°lise - URLs do Instagram expiram r√°pido!)
            if upload_to_cloud:
                workflow |= upload_to_cloud_task.s(bookmark_id, user_id)

            # 3. An√°lise com Gemini (usa cloud_video_url se dispon√≠vel)
            if analyze_video:
                # Gemini precisa dos dados da task anterior + bookmark_id + url
                # .s() cria signature parcial - primeiro arg vem da task anterior
                workflow |= analyze_video_gemini_task.s(bookmark_id, url)

            # 4. Processamento Gemini 3.0 Pro (recebe resultado anterior)
            if process_ai:
                workflow |= process_claude_task.s(bookmark_id, user_id)

            # 5. Gerar embedding (sempre roda ap√≥s processamento IA)
            if process_ai:
                workflow |= generate_embedding_task.s(bookmark_id, user_id)

            # 6. Cleanup final (sempre roda)
            workflow |= cleanup_and_notify_task.s(bookmark_id, user_id)

            # Executar workflow
            result = workflow.apply_async()

        else:
            # Se n√£o extrair metadados, s√≥ roda cleanup
            result = cleanup_and_notify_task.si(
                {"bookmark_id": bookmark_id, "message": "Nenhum processamento solicitado"},
                bookmark_id,
                user_id
            ).apply_async()

        # Log estruturado do pipeline (ordem de execu√ß√£o)
        pipeline_config = []
        if extract_metadata:
            pipeline_config.append("Metadata:‚úì")
        if upload_to_cloud:
            pipeline_config.append("Upload:‚úì")
        if analyze_video:
            pipeline_config.append("Gemini:‚úì")
        if process_ai:
            pipeline_config.append("Gemini Pro:‚úì")
            pipeline_config.append("Embedding:‚úì")

        logger.info(
            f"‚úÖ [PIPELINE] {bookmark_id} - CRIADO | "
            f"{' '.join(pipeline_config)} | Job: {self.request.id[:8]}"
        )

        return {
            "success": True,
            "bookmark_id": bookmark_id,
            "job_id": self.request.id,
            "pipeline_id": result.id
        }

    except Exception as e:
        logger.error(f"‚ùå [PIPELINE] {bookmark_id} - ERRO | {str(e)[:100]}")
        update_bookmark_status(bookmark_id, "failed", self.request.id, str(e))
        raise


# ============================================================================
# TASKS INDIVIDUAIS (implementa√ß√£o nas pr√≥ximas fases)
# ============================================================================

@celery_app.task(bind=True, name="tasks.extract_metadata_task", max_retries=3)
def extract_metadata_task(self, bookmark_id: str, url: str, user_id: str):
    """
    FASE 3.1: Extrair metadados com Apify
    - Scraping YouTube/Instagram/TikTok
    - Upload de thumbnail pra Supabase Storage
    - Salvar metadados no database
    """
    timer = TaskTimer("METADATA", bookmark_id)
    timer.start()

    try:
        # 1. Extrair metadados com Apify
        logger.debug(f"Chamando Apify para URL: {url[:60]}")
        loop = asyncio.get_event_loop()
        metadata = loop.run_until_complete(apify_service.extract_metadata(url))

        if not metadata:
            raise Exception("Apify retornou None - falha na extra√ß√£o de metadados")

        # 2. Upload de thumbnail pra Supabase Storage
        cloud_thumbnail_url = None
        if metadata.thumbnail_url and thumbnail_service:
            try:
                logger.info(f"üì∏ [TASKS.PY] Chamando ThumbnailService.upload_thumbnail()")
                logger.info(f"   - thumbnail_url (Instagram CDN): {metadata.thumbnail_url[:80]}...")
                logger.info(f"   - user_id: {user_id}")
                logger.info(f"   - bookmark_id: {bookmark_id}")

                cloud_thumbnail_url = loop.run_until_complete(
                    thumbnail_service.upload_thumbnail(
                        metadata.thumbnail_url,
                        user_id,
                        bookmark_id
                    )
                )

                if cloud_thumbnail_url:
                    logger.info(f"‚úÖ [TASKS.PY] Upload OK ‚Üí cloud_thumbnail_url: {cloud_thumbnail_url[:80]}...")
                else:
                    logger.warning(f"‚ö†Ô∏è [TASKS.PY] Upload retornou None")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Thumbnail upload falhou (n√£o cr√≠tico): {str(e)[:50]}")

        # 3. Salvar metadados no Supabase
        metadata_dict = metadata.dict()

        # üîç LOG CR√çTICO: Verificar se metadata.thumbnail_url est√° correto
        logger.info(f"üîç [TASKS.PY] ANTES DE SALVAR:")
        logger.info(f"   metadata.thumbnail_url (objeto): {metadata.thumbnail_url[:80] if metadata.thumbnail_url else 'NULL'}...")
        logger.info(f"   metadata.dict()['thumbnail_url']: {metadata_dict.get('thumbnail_url', 'NULL')[:80] if metadata_dict.get('thumbnail_url') else 'NULL'}...")
        logger.info(f"   cloud_thumbnail_url: {cloud_thumbnail_url[:80] if cloud_thumbnail_url else 'NULL'}...")

        update_data = {
            'title': metadata.title,
            'original_title': metadata.title,  # Imut√°vel
            'platform': metadata.platform.value if hasattr(metadata.platform, 'value') else str(metadata.platform),
            'thumbnail_url': metadata.thumbnail_url,
            'metadata': metadata_dict,  # JSON completo (Pydantic v1)
        }

        # Adicionar cloud_thumbnail_url se dispon√≠vel
        if cloud_thumbnail_url:
            update_data['cloud_thumbnail_url'] = cloud_thumbnail_url

        # Adicionar published_at se dispon√≠vel
        if metadata.published_at:
            update_data['published_at'] = metadata.published_at

        # Update no database
        supabase_client.table('bookmarks').update(update_data).eq('id', bookmark_id).execute()

        # üîç LOG CR√çTICO: Verificar o que foi salvo
        logger.info(f"üíæ [TASKS.PY] SALVO NO BANCO:")
        logger.info(f"   update_data['metadata']['thumbnail_url']: {update_data['metadata'].get('thumbnail_url', 'NULL')[:80] if update_data['metadata'].get('thumbnail_url') else 'NULL'}...")

        # Log consolidado de sucesso
        timer.success(
            T√≠tulo=metadata.title[:30] if metadata.title else "N/A",
            Thumb="‚úì" if cloud_thumbnail_url else "‚úó",
            Platform=update_data['platform']
        )

        # 4. Retornar dados para pr√≥xima task
        return {
            "bookmark_id": bookmark_id,
            "url": url,
            "user_id": user_id,
            "metadata_extracted": True,
            "title": metadata.title,
            "description": metadata.description or "",
            "hashtags": [tag.strip('#') for tag in (metadata.hashtags or [])],
            "top_comments": [
                {
                    "text": c.text,
                    "likes": c.likes,
                    "author": c.author
                } for c in (metadata.comments or [])[:200]  # Top 200 comments
            ],
            "cloud_thumbnail_url": cloud_thumbnail_url,
            "platform": update_data['platform']
        }

    except Exception as e:
        timer.error(f"Apify: {str(e)[:60]}")

        # Atualizar status no Supabase
        update_bookmark_status(bookmark_id, "failed", self.request.id, f"Erro na extra√ß√£o de metadados: {str(e)}")

        # Retry se for erro tempor√°rio
        if "timeout" in str(e).lower() or "connection" in str(e).lower():
            logger.warning(f"‚ö†Ô∏è Retry ap√≥s 30s (timeout/connection)")
            raise self.retry(exc=e, countdown=30)  # Retry ap√≥s 30s

        raise


@celery_app.task(bind=True, name="tasks.analyze_video_gemini_task", max_retries=2, time_limit=600)
def analyze_video_gemini_task(self, previous_result: dict, bookmark_id: str, url: str):
    """
    FASE 3.2: Analisar v√≠deo com Gemini Flash 2.5
    - Download tempor√°rio do v√≠deo (ou usa cloud_video_url se dispon√≠vel)
    - An√°lise multimodal (√°udio + visual + movimento)
    - Salvar transcri√ß√£o e an√°lise visual no database
    """
    timer = TaskTimer("GEMINI", bookmark_id)
    timer.start()

    temp_video_path = None

    try:
        # 1. Obter URL do v√≠deo para an√°lise
        video_url_for_analysis = None
        user_context = previous_result.get('user_context', '')

        # Verificar se j√° tem v√≠deo na cloud
        if 'cloud_video_url' in previous_result and previous_result['cloud_video_url']:
            video_url_for_analysis = previous_result['cloud_video_url']
            logger.debug(f"Usando v√≠deo da cloud: {video_url_for_analysis[:60]}")
        else:
            # Baixar v√≠deo temporariamente via Apify
            logger.debug("Baixando v√≠deo tempor√°rio via Apify")
            loop = asyncio.get_event_loop()

            # Detectar plataforma
            from models import Platform
            platform = apify_service.detect_platform(url)

            # Extrair URL direta do v√≠deo
            if platform == Platform.YOUTUBE:
                video_data = loop.run_until_complete(
                    apify_service.extract_video_download_url_youtube(url, quality="720p")
                )
            elif platform == Platform.INSTAGRAM:
                video_data = loop.run_until_complete(
                    apify_service.extract_video_download_url_instagram(url, quality="720p")
                )
            elif platform == Platform.TIKTOK:
                video_data = loop.run_until_complete(
                    apify_service.extract_video_download_url_tiktok(url, quality="720p")
                )
            else:
                raise Exception(f"Plataforma n√£o suportada: {platform}")

            if not video_data or not video_data.get('download_url'):
                raise Exception("Falha ao extrair URL do v√≠deo")

            video_url_for_analysis = video_data['download_url']

        # 2. Analisar v√≠deo com Gemini Flash 2.5
        loop = asyncio.get_event_loop()
        gemini_analysis = loop.run_until_complete(
            gemini_service.analyze_video(video_url_for_analysis, user_context)
        )

        if not gemini_analysis:
            raise Exception("Gemini retornou None")

        # 3. Salvar an√°lise no Supabase
        update_data = {
            'video_transcript': gemini_analysis.get('transcript', ''),
            'visual_analysis': gemini_analysis.get('visual_analysis', ''),
            'transcript_language': gemini_analysis.get('language', 'unknown'),
            'analyzed_at': datetime.utcnow().isoformat(),
        }

        supabase_client.table('bookmarks').update(update_data).eq('id', bookmark_id).execute()

        # Log consolidado de sucesso
        timer.success(
            Idioma=gemini_analysis.get('language', 'N/A'),
            FOOH="Sim" if gemini_analysis.get('is_fooh') else "N√£o",
            Transcript=f"{len(gemini_analysis.get('transcript', ''))} chars"
        )

        # 4. Retornar dados para pr√≥xima task
        return {
            **previous_result,
            "video_analyzed": True,
            "gemini_analysis": gemini_analysis,
            "transcript": gemini_analysis.get('transcript', ''),
            "visual_analysis": gemini_analysis.get('visual_analysis', ''),
            "editing_techniques": gemini_analysis.get('editing_techniques', []),
            "is_fooh": gemini_analysis.get('is_fooh', False),
            "language": gemini_analysis.get('language', 'unknown')
        }

    except Exception as e:
        timer.error(f"Gemini: {str(e)[:60]}")

        # Atualizar status no Supabase
        update_bookmark_status(bookmark_id, "failed", self.request.id, f"Erro na an√°lise Gemini: {str(e)}")

        # Retry se for erro tempor√°rio
        if "timeout" in str(e).lower() or "rate limit" in str(e).lower():
            logger.warning(f"‚ö†Ô∏è Retry ap√≥s 60s (timeout/rate limit)")
            raise self.retry(exc=e, countdown=60)  # Retry ap√≥s 60s

        raise

    finally:
        # Cleanup: deletar v√≠deo tempor√°rio se foi criado
        if temp_video_path and os.path.exists(temp_video_path):
            try:
                os.unlink(temp_video_path)
                logger.debug(f"V√≠deo tempor√°rio deletado: {temp_video_path}")
            except:
                pass


@celery_app.task(bind=True, name="tasks.process_claude_task", max_retries=2, time_limit=300)
def process_claude_task(self, previous_result: dict, bookmark_id: str, user_id: str):
    """
    FASE 3.3: Processar com Gemini 3.0 Pro
    - Juntar metadados + an√°lise Gemini 2.5 Flash + contexto do usu√°rio
    - Gerar tags, categorias, descri√ß√£o autom√°tica
    - Salvar no database
    """
    timer = TaskTimer("GEMINI_PRO", bookmark_id)
    timer.start()

    try:
        # 1. Extrair dados do previous_result
        title = previous_result.get('title', '')
        description = previous_result.get('description', '')
        hashtags = previous_result.get('hashtags', [])
        top_comments = previous_result.get('top_comments', [])
        gemini_analysis = previous_result.get('gemini_analysis', None)
        user_context = previous_result.get('user_context', '')

        if not title:
            raise Exception("T√≠tulo n√£o dispon√≠vel")

        logger.debug(f"Dados: t√≠tulo={title[:30]}, Gemini={'‚úì' if gemini_analysis else '‚úó'}, user_context={'‚úì' if user_context else '‚úó'}")

        # 2. Chamar Gemini 3.0 Pro
        loop = asyncio.get_event_loop()

        # Se tem an√°lise Gemini, usar novo m√©todo
        if gemini_analysis:
            result = loop.run_until_complete(
                claude_service.process_metadata_with_gemini(
                    title=title,
                    description=description,
                    hashtags=hashtags,
                    top_comments=top_comments,
                    gemini_analysis=gemini_analysis,
                    user_context=user_context
                )
            )
        else:
            # Fallback: usar m√©todo antigo (sem an√°lise de v√≠deo)
            logger.warning("‚ö†Ô∏è Sem an√°lise Gemini - usando fallback")
            result = loop.run_until_complete(
                claude_service.process_metadata_auto(
                    title=title,
                    description=description,
                    hashtags=hashtags,
                    top_comments=top_comments,
                    user_context=user_context
                )
            )

        if not result:
            raise Exception("Gemini Pro retornou None")

        # 3. Gerar Smart Title (t√≠tulo otimizado para recupera√ß√£o - metodologia CODE)
        smart_title = None
        try:
            logger.debug("üè∑Ô∏è Gerando smart title...")

            # Extrair visual_analysis se dispon√≠vel
            visual_analysis = previous_result.get('visual_analysis', None)

            smart_title = loop.run_until_complete(
                claude_service.generate_smart_title(
                    auto_description=result.get('auto_description', ''),
                    auto_tags=result.get('auto_tags', []),
                    user_context=user_context,
                    visual_analysis=visual_analysis
                )
            )

            if smart_title:
                logger.info(f"‚úÖ Smart title gerado: {smart_title[:60]}")
            else:
                logger.warning("‚ö†Ô∏è Smart title retornou None - usando t√≠tulo original")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao gerar smart title (n√£o cr√≠tico): {str(e)[:50]}")
            smart_title = None

        # 4. Salvar no Supabase

        update_data = {
            'auto_description': result.get('auto_description', ''),
            'auto_tags': result.get('auto_tags', []),
            'auto_categories': result.get('auto_categories', []),
            'relevance_score': result.get('relevance_score', 0.5),
            'ai_processed': True,
        }

        # Adicionar smart_title se foi gerado
        if smart_title:
            update_data['smart_title'] = smart_title

        # Adicionar filtered_comments se dispon√≠vel
        if 'filtered_comments' in result:
            # Extrair apenas campos necess√°rios (economizar espa√ßo no DB)
            filtered_comments_simple = [
                {
                    'text': c.get('text', ''),
                    'likes': c.get('likes', 0)
                }
                for c in result['filtered_comments'][:50]  # Top 50
            ]
            update_data['filtered_comments'] = filtered_comments_simple

        supabase_client.table('bookmarks').update(update_data).eq('id', bookmark_id).execute()

        # Log consolidado de sucesso
        timer.success(
            Tags=len(result.get('auto_tags', [])),
            Categorias=len(result.get('auto_categories', [])),
            Relev√¢ncia=f"{result.get('relevance_score', 0):.2f}",
            SmartTitle="‚úì" if smart_title else "‚úó"
        )

        # 4. Retornar dados para pr√≥xima task
        return {
            **previous_result,
            "ai_processed": True,
            "auto_description": result.get('auto_description', ''),
            "auto_tags": result.get('auto_tags', []),
            "auto_categories": result.get('auto_categories', []),
            "relevance_score": result.get('relevance_score', 0.5),
        }

    except Exception as e:
        timer.error(f"Gemini Pro: {str(e)[:60]}")

        # Atualizar status no Supabase
        update_bookmark_status(bookmark_id, "failed", self.request.id, f"Erro no processamento Gemini Pro: {str(e)}")

        # Retry se for erro tempor√°rio
        if "timeout" in str(e).lower() or "rate limit" in str(e).lower():
            logger.warning(f"‚ö†Ô∏è Retry ap√≥s 45s (timeout/rate limit)")
            raise self.retry(exc=e, countdown=45)  # Retry ap√≥s 45s

        raise


@celery_app.task(bind=True, name="tasks.upload_to_cloud_task", max_retries=2, time_limit=900)
def upload_to_cloud_task(self, previous_result: dict, bookmark_id: str, user_id: str):
    """
    FASE 3.4: Upload de v√≠deo pra Supabase Storage
    - Baixa v√≠deo via Apify
    - Upload do v√≠deo pra Supabase Storage
    - Gerar Signed URL
    - Atualizar database
    """
    logger.info(f"‚òÅÔ∏è Upload pra cloud - Bookmark: {bookmark_id}")

    temp_video_path = None

    try:
        url = previous_result.get('url')
        if not url:
            raise Exception("URL n√£o dispon√≠vel para upload de v√≠deo")

        # 1. Baixar v√≠deo via Apify
        logger.info("‚¨áÔ∏è Baixando v√≠deo via Apify...")

        loop = asyncio.get_event_loop()
        from models import Platform
        platform = apify_service.detect_platform(url)

        # Extrair URL direta
        if platform == Platform.YOUTUBE:
            video_data = loop.run_until_complete(
                apify_service.extract_video_download_url_youtube(url, quality="720p")
            )
        elif platform == Platform.INSTAGRAM:
            video_data = loop.run_until_complete(
                apify_service.extract_video_download_url_instagram(url, quality="720p")
            )
        elif platform == Platform.TIKTOK:
            video_data = loop.run_until_complete(
                apify_service.extract_video_download_url_tiktok(url, quality="720p")
            )
        else:
            raise Exception(f"Plataforma n√£o suportada: {platform}")

        if not video_data or not video_data.get('download_url'):
            raise Exception("Falha ao extrair URL do v√≠deo")

        download_url = video_data['download_url']
        logger.info(f"‚úÖ URL obtida: {download_url[:80]}...")

        # 2. Baixar v√≠deo para arquivo tempor√°rio
        import httpx
        logger.info("‚¨áÔ∏è Baixando v√≠deo...")

        temp_video_path = f"/tmp/{bookmark_id}.mp4"
        with httpx.stream("GET", download_url, timeout=180.0, follow_redirects=True) as response:
            response.raise_for_status()
            with open(temp_video_path, "wb") as f:
                for chunk in response.iter_bytes(chunk_size=8192):
                    f.write(chunk)

        file_size_bytes = os.path.getsize(temp_video_path)
        file_size_mb = file_size_bytes / (1024 * 1024)
        logger.info(f"‚úÖ V√≠deo baixado: {file_size_mb:.2f}MB")

        # 3. Upload para Supabase Storage
        logger.info("‚òÅÔ∏è Fazendo upload para Supabase Storage...")

        cloud_path = f"{user_id}/{bookmark_id}.mp4"
        with open(temp_video_path, "rb") as video_file:
            supabase_client.storage.from_("user-videos").upload(
                path=cloud_path,
                file=video_file,
                file_options={"content-type": "video/mp4", "upsert": "true"}
            )

        logger.info(f"‚úÖ Upload conclu√≠do: {cloud_path}")

        # 4. Gerar Signed URL (v√°lida por 1 ano)
        signed_url_data = supabase_client.storage.from_("user-videos").create_signed_url(
            path=cloud_path,
            expires_in=31536000  # 1 ano
        )

        cloud_url = signed_url_data.get("signedURL") if signed_url_data else None
        if not cloud_url:
            raise Exception("Falha ao gerar Signed URL")

        logger.info(f"‚úÖ Signed URL gerada: {cloud_url[:80]}...")

        # 5. Atualizar database
        logger.info("üíæ Atualizando Supabase com cloud URL...")

        supabase_client.table('bookmarks').update({
            'cloud_video_url': cloud_url,
            'cloud_upload_status': 'completed',
            'cloud_uploaded_at': datetime.utcnow().isoformat(),
            'cloud_file_size_bytes': file_size_bytes,
        }).eq('id', bookmark_id).execute()

        logger.info(f"‚úÖ Database atualizado - Bookmark: {bookmark_id}")

        return {
            **previous_result,
            "cloud_uploaded": True,
            "cloud_video_url": cloud_url,
            "cloud_file_size_mb": file_size_mb
        }

    except Exception as e:
        logger.error(f"‚ùå Erro ao fazer upload pra cloud - Bookmark: {bookmark_id}, Erro: {str(e)}")

        # Atualizar status no Supabase
        supabase_client.table('bookmarks').update({
            'cloud_upload_status': 'failed',
        }).eq('id', bookmark_id).execute()

        update_bookmark_status(bookmark_id, "failed", self.request.id, f"Erro no upload cloud: {str(e)}")

        # Retry se for erro tempor√°rio
        if "timeout" in str(e).lower() or "connection" in str(e).lower():
            raise self.retry(exc=e, countdown=90)  # Retry ap√≥s 90s

        raise

    finally:
        # Cleanup: deletar v√≠deo tempor√°rio
        if temp_video_path and os.path.exists(temp_video_path):
            try:
                os.unlink(temp_video_path)
                logger.info(f"üóëÔ∏è V√≠deo tempor√°rio deletado: {temp_video_path}")
            except:
                pass


@celery_app.task(bind=True, name="tasks.generate_embedding_task", max_retries=2, time_limit=60)
def generate_embedding_task(self, previous_result: dict, bookmark_id: str, user_id: str):
    """
    FASE 3.4: Gerar embedding sem√¢ntico
    - Combina smart_title + auto_tags + auto_categories + transcription + multimodal_analysis
    - Gera vetor de 768 dimens√µes via Gemini Embedding API
    - Salva no Supabase (coluna embedding)
    """
    timer = TaskTimer("EMBEDDING", bookmark_id)
    timer.start()

    try:
        # 1. Buscar dados do bookmark no Supabase
        logger.debug(f"Buscando dados do bookmark para embedding...")

        response = supabase_client.table('bookmarks').select(
            'smart_title, auto_tags, auto_categories, video_transcript, visual_analysis'
        ).eq('id', bookmark_id).single().execute()

        if not response.data:
            raise Exception("Bookmark n√£o encontrado no Supabase")

        bookmark = response.data

        # 2. Gerar embedding
        logger.info(f"üìä Gerando embedding...")

        embedding = embedding_service.generate_from_bookmark_dict(bookmark)

        if not embedding:
            raise Exception("Embedding service retornou None")

        # 3. Salvar no Supabase
        supabase_client.table('bookmarks').update({
            'embedding': embedding
        }).eq('id', bookmark_id).execute()

        # Log consolidado de sucesso
        timer.success(
            Dimens√µes=len(embedding),
            Campos=f"SmartTitle={'‚úì' if bookmark.get('smart_title') else '‚úó'} Tags={len(bookmark.get('auto_tags', []))}"
        )

        # 4. Retornar dados para pr√≥xima task
        return {
            **previous_result,
            "embedding_generated": True,
            "embedding_dimensions": len(embedding)
        }

    except Exception as e:
        timer.error(f"Embedding: {str(e)[:60]}")

        # Retry se for erro tempor√°rio
        if "timeout" in str(e).lower() or "rate limit" in str(e).lower():
            logger.warning(f"‚ö†Ô∏è Retry ap√≥s 30s (timeout/rate limit)")
            raise self.retry(exc=e, countdown=30)

        # N√£o falhar o bookmark inteiro se embedding falhar
        logger.warning(f"‚ö†Ô∏è Embedding falhou (n√£o cr√≠tico) - continuando pipeline")
        return {
            **previous_result,
            "embedding_generated": False,
            "embedding_error": str(e)[:100]
        }


@celery_app.task(bind=True, name="tasks.cleanup_and_notify_task")
def cleanup_and_notify_task(self, previous_result: dict, bookmark_id: str, user_id: str):
    """
    FASE 3.5: Cleanup e notifica√ß√£o
    - Deletar arquivos tempor√°rios
    - Atualizar status: processing ‚Üí completed
    - Notificar celular (opcional: push notification)
    """
    logger.info(f"üßπ Cleanup e notifica√ß√£o - Bookmark: {bookmark_id}")

    try:
        # 1. Cleanup de arquivos tempor√°rios
        logger.info("üóëÔ∏è Limpando arquivos tempor√°rios...")

        temp_patterns = [
            f"/tmp/{bookmark_id}*",
            f"/tmp/video_{bookmark_id}*",
            f"/tmp/thumb_{bookmark_id}*",
        ]

        import glob
        for pattern in temp_patterns:
            for file_path in glob.glob(pattern):
                try:
                    os.unlink(file_path)
                    logger.debug(f"Deletado: {file_path}")
                except:
                    pass

        # 2. Atualizar status final no Supabase
        update_bookmark_status(bookmark_id, "completed", self.request.id, error_message=None)

        # 3. LOG RESUMO FINAL (uma linha com tudo - ordem de execu√ß√£o)
        pipeline_summary = []
        if previous_result.get('metadata_extracted'):
            pipeline_summary.append("Metadata:‚úì")
        if previous_result.get('cloud_uploaded'):
            pipeline_summary.append("Upload:‚úì")
        if previous_result.get('video_analyzed'):
            pipeline_summary.append("Gemini:‚úì")
        if previous_result.get('ai_processed'):
            pipeline_summary.append("Gemini Pro:‚úì")

        tags_count = len(previous_result.get('auto_tags', []))
        cats_count = len(previous_result.get('auto_categories', []))

        logger.info(
            f"üéâ [PIPELINE] {bookmark_id} - COMPLETO | "
            f"{' '.join(pipeline_summary)} | "
            f"Tags: {tags_count} | Categorias: {cats_count}"
        )

        # 4. (Opcional) Enviar notifica√ß√£o push
        # TODO: Implementar quando tiver Firebase Cloud Messaging

        return {
            **previous_result,
            "cleanup_done": True,
            "status": "completed",
            "message": "Processamento conclu√≠do com sucesso!"
        }

    except Exception as e:
        logger.error(f"‚ùå Erro no cleanup - Bookmark: {bookmark_id}, Erro: {str(e)}")
        # N√£o falha o processamento inteiro por erro no cleanup
        return {
            **previous_result,
            "cleanup_done": False,
            "status": "completed",  # Marca como completed mesmo assim
            "message": f"Processamento conclu√≠do mas cleanup falhou: {str(e)}"
        }


# ============================================================================
# AUTO-SYNC (cron job di√°rio)
# ============================================================================

@celery_app.task(bind=True, name="tasks.auto_sync_incomplete_bookmarks_task")
def auto_sync_incomplete_bookmarks_task(self):
    """
    FASE 5: Auto-sync de bookmarks incompletos (cron job di√°rio)
    - Roda √†s 3h da manh√£
    - Processa TODOS os bookmarks incompletos de TODOS os usu√°rios
    - Paraleliza processamento (batch de 10 jobs simult√¢neos)
    """
    logger.info("üîÑ Iniciando auto-sync di√°rio de bookmarks incompletos")

    try:
        # 1. Query para encontrar bookmarks incompletos
        logger.info("üîç Buscando bookmarks incompletos no Supabase...")

        # Query otimizada - considera m√∫ltiplos crit√©rios de incompletude
        query = supabase_client.table('bookmarks').select('id, url, user_id, metadata, auto_tags, cloud_video_url, video_transcript, visual_analysis')

        # Filtros (OR logic):
        # - Sem metadados b√°sicos
        # - Sem processamento de IA
        # - Com v√≠deo na cloud mas sem an√°lise multimodal
        # - Status failed ou pending
        incomplete_bookmarks = query.or_(
            'metadata.is.null,'
            'auto_tags.is.null,'
            'processing_status.eq.failed,'
            'processing_status.eq.pending,'
            'and(cloud_video_url.not.is.null,video_transcript.is.null)'
        ).execute()

        bookmarks = incomplete_bookmarks.data if incomplete_bookmarks else []
        total = len(bookmarks)

        logger.info(f"üìä Encontrados {total} bookmarks incompletos")

        if total == 0:
            logger.info("‚úÖ Nenhum bookmark incompleto - auto-sync n√£o necess√°rio")
            return {
                "success": True,
                "processed": 0,
                "message": "Nenhum bookmark incompleto encontrado"
            }

        # 2. Processar em batches de 10 (evitar sobrecarga)
        batch_size = 10
        batches = [bookmarks[i:i + batch_size] for i in range(0, total, batch_size)]
        total_batches = len(batches)

        logger.info(f"üì¶ Processando em {total_batches} batches de at√© {batch_size} bookmarks")

        processed_count = 0
        failed_count = 0

        for batch_idx, batch in enumerate(batches):
            logger.info(f"‚öôÔ∏è Processando batch {batch_idx + 1}/{total_batches} ({len(batch)} bookmarks)")

            # Criar jobs em paralelo para este batch
            jobs = []
            for bookmark in batch:
                bookmark_id = bookmark['id']
                url = bookmark['url']
                user_id = bookmark['user_id']

                # Determinar o que processar
                has_metadata = bookmark.get('metadata') is not None
                has_ai = bookmark.get('auto_tags') is not None and len(bookmark.get('auto_tags', [])) > 0
                has_cloud_video = bookmark.get('cloud_video_url') is not None and bookmark['cloud_video_url']
                has_analysis = bookmark.get('video_transcript') is not None and bookmark['video_transcript']

                # Configurar par√¢metros
                extract_metadata = not has_metadata
                analyze_video = has_cloud_video and not has_analysis
                process_ai = not has_ai
                upload_to_cloud = False  # N√£o faz upload autom√°tico (economiza banda)

                logger.info(f"üìù {bookmark_id}: metadata={has_metadata}, ai={has_ai}, cloud={has_cloud_video}, analysis={has_analysis}")

                # Enfileirar job
                try:
                    job = process_bookmark_complete_task.apply_async(
                        kwargs={
                            'bookmark_id': bookmark_id,
                            'url': url,
                            'user_id': user_id,
                            'extract_metadata': extract_metadata,
                            'analyze_video': analyze_video,
                            'process_ai': process_ai,
                            'upload_to_cloud': upload_to_cloud,
                        },
                        retry=False  # N√£o retry autom√°tico (vai tentar no pr√≥ximo dia)
                    )
                    jobs.append(job)
                    processed_count += 1
                except Exception as e:
                    logger.error(f"‚ùå Erro ao enfileirar {bookmark_id}: {str(e)}")
                    failed_count += 1

            logger.info(f"‚úÖ Batch {batch_idx + 1} enfileirado: {len(jobs)} jobs criados")

        logger.info(f"üéâ Auto-sync conclu√≠do: {processed_count} processados, {failed_count} falharam")

        return {
            "success": True,
            "total_found": total,
            "processed": processed_count,
            "failed": failed_count,
            "batches": total_batches,
            "message": f"Auto-sync conclu√≠do: {processed_count}/{total} bookmarks enfileirados"
        }

    except Exception as e:
        logger.error(f"‚ùå Erro no auto-sync di√°rio: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "message": "Erro no auto-sync di√°rio"
        }


@celery_app.task(bind=True, name="tasks.cleanup_temp_files_task")
def cleanup_temp_files_task(self):
    """
    Cleanup de arquivos tempor√°rios (roda a cada 6 horas)
    - Deleta v√≠deos tempor√°rios mais antigos que 24h
    - Deleta thumbnails tempor√°rios
    """
    logger.info("üßπ Cleanup de arquivos tempor√°rios")

    # TODO: Implementar
    return {
        "success": True,
        "files_deleted": 0
    }


# ============================================================================
# FUN√á√ïES AUXILIARES
# ============================================================================

def update_bookmark_status(
    bookmark_id: str,
    status: str,
    job_id: Optional[str] = None,
    error_message: Optional[str] = None
):
    """
    Atualiza status de processamento no Supabase

    Args:
        bookmark_id: UUID do bookmark
        status: queued | processing | completed | failed
        job_id: ID do job Celery
        error_message: Mensagem de erro (se falhou)
    """
    # TODO: Implementar na FASE 6.1 (ap√≥s migration de campos)
    logger.info(f"üìù Status atualizado: {bookmark_id} ‚Üí {status}")
    pass
